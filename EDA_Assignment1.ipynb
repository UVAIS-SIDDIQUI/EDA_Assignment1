{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf4ff038-2d34-46ae-bcc2-f4e9046a8746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What are the key features of the wine quality data set? Discuss the importance of each feature in \n",
    "# predicting the quality of wine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a6eee-cc0a-4070-a801-b1de1ee41dac",
   "metadata": {},
   "source": [
    "ANS =  The wine quality data set is a popular dataset in machine learning and data science used for regression and classification tasks. It contains various chemical properties of wine along with a quality rating. The key features of the wine quality data set typically include:\n",
    "\n",
    "Fixed Acidity: Refers to acids that do not evaporate easily (e.g., tartaric acid). It's an essential component that influences the taste and longevity of wine.\n",
    "\n",
    "Volatile Acidity: Measures the amount of acetic acid in wine, which can lead to an unpleasant vinegar taste. High levels of volatile acidity are undesirable.\n",
    "\n",
    "Citric Acid: Acts as a preservative and can add a fresh flavor to the wine. It can enhance the taste and improve the stability of wine.\n",
    "\n",
    "Residual Sugar: The amount of sugar remaining after fermentation stops. This determines the sweetness of the wine and can influence its overall balance and body.\n",
    "\n",
    "Chlorides: Represents the amount of salt in the wine. High chloride content can negatively affect the taste, making it salty.\n",
    "\n",
    "Free Sulfur Dioxide: The free form of SO2, which prevents microbial growth and oxidation. It's an important preservative used in wine.\n",
    "\n",
    "Total Sulfur Dioxide: The sum of free and bound forms of SO2, indicating the total amount of preservative in the wine. It ensures the wine's stability and longevity.\n",
    "\n",
    "Density: Related to the alcohol content and residual sugar. It's a measure that can help differentiate between different types of wines.\n",
    "\n",
    "pH: Describes the acidity or basicity of the wine. pH affects the wine's taste, color, and stability.\n",
    "\n",
    "Sulphates: Acts as an antioxidant and antimicrobial agent, contributing to the wine's overall stability and shelf life.\n",
    "\n",
    "Alcohol: The alcohol content of the wine, which significantly affects its body, flavor, and quality.\n",
    "\n",
    "Quality: The target variable, typically rated on a scale (e.g., 0-10), representing the overall quality of the wine as assessed by experts.\n",
    "\n",
    "Importance of Each Feature in Predicting Wine Quality\n",
    "Fixed Acidity and pH: These features are crucial for determining the basic structure and taste profile of the wine. Wines with balanced acidity are generally more appealing.\n",
    "\n",
    "Volatile Acidity: High levels are usually a negative indicator as they can cause an unpleasant taste. Hence, it's a critical factor in quality assessment.\n",
    "\n",
    "Citric Acid: Although present in smaller quantities, it can enhance flavor and preservation, contributing positively to wine quality.\n",
    "\n",
    "Residual Sugar: Affects the sweetness and overall balance. Depending on the type of wine, varying levels of residual sugar can either enhance or detract from quality.\n",
    "\n",
    "Chlorides: High levels can spoil the taste, making this a negative indicator if too high.\n",
    "\n",
    "Sulfur Dioxide Levels (Free and Total): Essential for preservation but must be balanced. Too much can affect taste and health, while too little can lead to spoilage.\n",
    "\n",
    "Density: Indicates alcohol and sugar content, both of which are crucial for the wine's body and taste profile.\n",
    "\n",
    "Sulphates: Important for stabilization and preservation, influencing the wine's shelf life and quality.\n",
    "\n",
    "Alcohol: Higher alcohol content can enhance body and warmth, contributing to the wine's overall sensory experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83c176bf-7417-4e5d-91dc-290f0e25dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Q2 How did you handle missing data in the wine quality data set during the feature engineering process? \n",
    "# Discuss the advantages and disadvantages of different imputation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3452c6-64fa-4521-af2f-b2285147826e",
   "metadata": {},
   "source": [
    "ANS = Handling missing data is a crucial step in the feature engineering process as it ensures the quality and accuracy of the predictive models. In the context of the wine quality dataset, here are some common techniques for handling missing data and their respective advantages and disadvantages:\n",
    "\n",
    "Techniques for Handling Missing Data\n",
    "Removing Rows with Missing Values\n",
    "\n",
    "Advantages:\n",
    "Simple and straightforward to implement.\n",
    "Ensures that only complete and consistent data is used for analysis.\n",
    "Disadvantages:\n",
    "Reduces the dataset size, which can be detrimental if the dataset is already small.\n",
    "May lead to biased results if the missing data is not randomly distributed.\n",
    "Mean/Median/Mode Imputation\n",
    "\n",
    "Mean Imputation: Replacing missing values with the mean of the column.\n",
    "Median Imputation: Replacing missing values with the median of the column.\n",
    "Mode Imputation: Replacing missing values with the mode (most frequent value) of the column.\n",
    "Advantages:\n",
    "Simple to implement and computationally efficient.\n",
    "Maintains the overall distribution of the data.\n",
    "Disadvantages:\n",
    "Does not account for the variability in the data, potentially reducing the variance.\n",
    "Can introduce bias, especially if the data has outliers or is skewed.\n",
    "K-Nearest Neighbors (KNN) Imputation\n",
    "\n",
    "Imputes missing values based on the values of the nearest neighbors.\n",
    "Advantages:\n",
    "Takes into account the relationships between different features.\n",
    "Can be more accurate than simple imputation methods.\n",
    "Disadvantages:\n",
    "Computationally intensive, especially for large datasets.\n",
    "The choice of k (number of neighbors) can significantly impact the results.\n",
    "Multivariate Imputation by Chained Equations (MICE)\n",
    "\n",
    "Uses the relationships between different features to iteratively impute missing values.\n",
    "Advantages:\n",
    "Accounts for the correlations between variables.\n",
    "Can provide more accurate and robust imputed values.\n",
    "Disadvantages:\n",
    "Computationally intensive and complex to implement.\n",
    "Requires careful tuning and validation.\n",
    "Using Algorithms That Support Missing Values\n",
    "\n",
    "Some machine learning algorithms (e.g., XGBoost, LightGBM) can handle missing values natively.\n",
    "Advantages:\n",
    "Eliminates the need for explicit imputation.\n",
    "Can leverage the information inherent in the presence of missing values.\n",
    "Disadvantages:\n",
    "Limited to specific algorithms.\n",
    "The performance can vary depending on the dataset and the proportion of missing values.\n",
    "Implementation Considerations\n",
    "When handling missing data in the wine quality dataset, the choice of imputation technique depends on several factors, including the amount and pattern of missing data, the computational resources available, and the specific requirements of the predictive model. Here are some general steps for handling missing data during feature engineering:\n",
    "\n",
    "Exploratory Data Analysis (EDA): Understand the extent and pattern of missing data.\n",
    "Choosing an Imputation Method: Based on EDA, select an appropriate imputation technique.\n",
    "Implementing Imputation: Apply the chosen method to impute missing values.\n",
    "Validation: Evaluate the impact of imputation on model performance.\n",
    "Example Scenario\n",
    "For instance, if the wine quality dataset has a small percentage of missing values randomly distributed, mean/median/mode imputation might be a suitable choice due to its simplicity and efficiency. However, if the dataset has a significant amount of missing values or if the missing data is not random, more sophisticated techniques like KNN or MICE might be preferable to ensure the accuracy and robustness of the imputed values.\n",
    "\n",
    "Conclusion\n",
    "Handling missing data is a critical step in the feature engineering process. The choice of imputation technique can significantly impact the performance of predictive models. Understanding the advantages and disadvantages of each method allows data scientists to make informed decisions and achieve the best possible outcomes for their analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d283f1e-4d6c-4857-b5c1-f8226d4caf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What are the key factors that affect students' performance in exams? How would you go about \n",
    "# analyzing these factors using statistical techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5b20ab-33bb-4be7-ab0a-25a83d3468e6",
   "metadata": {},
   "source": [
    "ANS = Analyzing the factors affecting students' performance in exams involves understanding a variety of potential influences, including demographic, socio-economic, behavioral, and educational factors. Here’s a detailed approach to identifying and analyzing these factors using statistical techniques:\n",
    "\n",
    "Key Factors Affecting Students' Performance\n",
    "Demographic Factors:\n",
    "\n",
    "Age\n",
    "Gender\n",
    "Ethnicity\n",
    "Socio-Economic Factors:\n",
    "\n",
    "Family income\n",
    "Parents' education level\n",
    "Access to educational resources (e.g., books, internet)\n",
    "Behavioral Factors:\n",
    "\n",
    "Study habits\n",
    "Attendance\n",
    "Participation in extracurricular activities\n",
    "Time management\n",
    "Psychological Factors:\n",
    "\n",
    "Motivation\n",
    "Stress levels\n",
    "Self-esteem\n",
    "Educational Factors:\n",
    "\n",
    "Quality of teaching\n",
    "Class size\n",
    "Curriculum\n",
    "Availability of tutoring or additional help\n",
    "Analyzing These Factors Using Statistical Techniques\n",
    "Data Collection:\n",
    "\n",
    "Gather data from student records, surveys, and standardized tests.\n",
    "Ensure the dataset includes relevant features such as demographics, socio-economic background, behavioral patterns, and exam scores.\n",
    "Exploratory Data Analysis (EDA):\n",
    "\n",
    "Descriptive Statistics: Summarize the main characteristics of the data using mean, median, mode, standard deviation, etc.\n",
    "Visualization: Use histograms, box plots, scatter plots, and bar charts to visualize the distribution and relationships between variables.\n",
    "Correlation Analysis:\n",
    "\n",
    "Calculate correlation coefficients (Pearson or Spearman) to measure the strength and direction of relationships between variables.\n",
    "Create a correlation matrix to identify pairs of variables with strong correlations.\n",
    "Hypothesis Testing:\n",
    "\n",
    "Use t-tests or ANOVA to compare the means of exam scores across different groups (e.g., gender, socio-economic status).\n",
    "Conduct chi-square tests for categorical variables to see if there are significant associations between categories (e.g., participation in extracurricular activities and pass/fail rates).\n",
    "Regression Analysis:\n",
    "\n",
    "Linear Regression: Model the relationship between exam scores (dependent variable) and one or more independent variables (e.g., study hours, family income).\n",
    "Multiple Regression: Include multiple predictors to understand the combined effect of different factors on exam performance.\n",
    "Logistic Regression: If the outcome variable is categorical (e.g., pass/fail), use logistic regression to model the probability of a particular outcome.\n",
    "Multivariate Analysis:\n",
    "\n",
    "Principal Component Analysis (PCA): Reduce the dimensionality of the data to identify key components that explain the most variance in exam performance.\n",
    "Cluster Analysis: Group students into clusters based on similar characteristics to identify patterns and common factors affecting performance.\n",
    "Machine Learning Techniques:\n",
    "\n",
    "Decision Trees and Random Forests: Identify important features and their interactions by constructing tree-based models.\n",
    "Support Vector Machines (SVM): Classify students into different performance categories based on various predictors.\n",
    "Neural Networks: Capture complex non-linear relationships between predictors and exam performance.\n",
    "Steps for Analyzing the Factors\n",
    "Preprocessing:\n",
    "\n",
    "Handle missing data through imputation or removal.\n",
    "Encode categorical variables using techniques such as one-hot encoding or label encoding.\n",
    "Normalize or standardize numerical features if needed.\n",
    "Feature Selection:\n",
    "\n",
    "Use techniques like forward selection, backward elimination, or regularization (Lasso, Ridge) to select the most important features.\n",
    "Model Building:\n",
    "\n",
    "Split the data into training and testing sets.\n",
    "Train different models using the training set and evaluate their performance using the testing set.\n",
    "Use metrics such as R-squared, Mean Squared Error (MSE), accuracy, precision, recall, and F1-score to assess model performance.\n",
    "Interpretation and Reporting:\n",
    "\n",
    "Interpret the coefficients of regression models to understand the impact of each factor.\n",
    "Visualize the results using plots and charts to convey insights effectively.\n",
    "Summarize findings in a report, highlighting key factors and their influence on student performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ffba70-9b4d-4cc8-9d47-3d765496ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Describe the process of feature engineering in the context of the student performance data set. How \n",
    "# did you select and transform the variables for your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe75659-7412-4800-84fb-097daedf8713",
   "metadata": {},
   "source": [
    "ANS = Feature engineering is a critical step in preparing the student performance dataset for modeling. This process involves selecting, transforming, and creating new features that can improve the predictive power of the model. Here's a detailed outline of the feature engineering process in the context of the student performance dataset:\n",
    "\n",
    "1. Understanding the Data\n",
    "Start by understanding the dataset, which typically includes information on demographics, socio-economic background, behavioral patterns, and educational metrics. Common features might include:\n",
    "\n",
    "Demographic: Age, Gender\n",
    "Socio-Economic: Family income, Parents' education level, Access to educational resources\n",
    "Behavioral: Study hours, Attendance, Participation in extracurricular activities, Time spent on homework\n",
    "Educational: Test scores, Grades, Teacher assessments\n",
    "2. Data Cleaning\n",
    "Handle Missing Values: Identify and treat missing values using imputation techniques such as mean/mode/median imputation, KNN imputation, or removing rows with missing values.\n",
    "Remove Duplicates: Ensure there are no duplicate entries in the dataset.\n",
    "Correct Data Types: Ensure all features have the correct data types (e.g., numerical, categorical).\n",
    "3. Exploratory Data Analysis (EDA)\n",
    "Descriptive Statistics: Summarize the dataset to understand the distribution of each feature.\n",
    "Visualization: Use plots (histograms, box plots, scatter plots) to visualize the relationships between features and the target variable (e.g., exam scores).\n",
    "4. Feature Selection\n",
    "Identify the most relevant features that influence student performance:\n",
    "\n",
    "Correlation Analysis: Use correlation matrices to identify relationships between features and the target variable. Select features with strong correlations.\n",
    "Statistical Tests: Use ANOVA or t-tests to determine the significance of categorical features.\n",
    "Domain Knowledge: Leverage educational insights to select features that are theoretically important.\n",
    "5. Feature Transformation\n",
    "Transform the selected features to improve model performance:\n",
    "\n",
    "Scaling: Normalize or standardize numerical features to ensure they are on a comparable scale. Techniques include Min-Max scaling and Z-score standardization.\n",
    "Encoding Categorical Variables: Convert categorical variables into numerical formats using:\n",
    "One-Hot Encoding: For nominal categorical variables (e.g., Gender).\n",
    "Ordinal Encoding: For ordinal categorical variables (e.g., Education levels).\n",
    "Polynomial Features: Create polynomial features to capture non-linear relationships.\n",
    "Interaction Features: Combine features to capture interactions between them (e.g., Study hours * Attendance).\n",
    "Log Transformation: Apply log transformation to skewed features to make them more normally distributed.\n",
    "Binning: Convert continuous variables into categorical bins (e.g., Age groups).\n",
    "6. Creating New Features\n",
    "Generate new features that can provide additional insights:\n",
    "\n",
    "Aggregated Features: Sum or average related features (e.g., total study hours per week).\n",
    "Behavioral Patterns: Create features based on behavioral insights (e.g., consistent study habits).\n",
    "Domain-Specific Features: Based on educational theories (e.g., parental involvement index).\n",
    "7. Dimensionality Reduction\n",
    "Reduce the number of features to prevent overfitting and improve model efficiency:\n",
    "\n",
    "Principal Component Analysis (PCA): Transform features into a lower-dimensional space while retaining most of the variance.\n",
    "Feature Importance from Models: Use models like Random Forest or Gradient Boosting to determine feature importance and select the top features.\n",
    "8. Model Building and Evaluation\n",
    "After feature engineering, split the dataset into training and testing sets and build the model:\n",
    "\n",
    "Train Models: Train different models (e.g., Linear Regression, Decision Trees, Random Forest) using the engineered features.\n",
    "Evaluate Performance: Use metrics such as R-squared, Mean Squared Error (MSE), accuracy, precision, recall, and F1-score to evaluate model performance on the testing set.\n",
    "Feature Impact: Interpret model coefficients or feature importance scores to understand the impact of each feature on the target variable.\n",
    "Example Scenario\n",
    "Assume the student performance dataset includes features like age, gender, parental education, study hours, attendance, and past grades. Here’s how you might engineer these features:\n",
    "\n",
    "Scaling: Standardize numerical features like age, study hours, and past grades.\n",
    "Encoding: One-hot encode categorical features like gender and parental education.\n",
    "Interaction Features: Create an interaction feature between study hours and attendance.\n",
    "Behavioral Features: Aggregate study hours and attendance to create a ‘total engagement’ feature.\n",
    "Dimensionality Reduction: Apply PCA to reduce the dimensionality if there are too many features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d050864-ded8-49ca-9ae9-af2881d6d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Load the wine quality data set and perform exploratory data analysis (EDA) to identify the distribution \n",
    "# of each feature. Which feature(s) exhibit non-normality, and what transformations could be applied to \n",
    "# these features to improve normality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ef4fd7-6a46-4d17-858d-37445b5f51be",
   "metadata": {},
   "source": [
    "Since I don't have direct access to a dataset, I'll guide you through the process of performing exploratory data analysis (EDA) and identifying non-normal features using Python. You can follow these steps on your local machine:\n",
    "\n",
    "Loading the Data\n",
    "First, you need to load the wine quality dataset. Assuming it's in a CSV file, you can use the pandas library to load it.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('winequality.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "Exploratory Data Analysis (EDA)\n",
    "Next, you will perform EDA to understand the distribution of each feature. This can be done using descriptive statistics and visualizations\n",
    "\n",
    "\n",
    "Exploratory Data Analysis (EDA)\n",
    "Next, you will perform EDA to understand the distribution of each feature. This can be done using descriptive statistics and visualizations\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Descriptive statistics\n",
    "print(data.describe())\n",
    "\n",
    "# Visualizations\n",
    "# Histograms for each feature\n",
    "data.hist(bins=20, figsize=(15, 10))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Identifying Non-Normal Features\n",
    "To identify non-normal features, you can use visual methods like histograms and Q-Q plots, as well as statistical tests like the Shapiro-Wilk test.\n",
    "\n",
    "\n",
    "from scipy.stats import shapiro, probplot\n",
    "\n",
    "# Q-Q plots and Shapiro-Wilk test\n",
    "for column in data.columns:\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Q-Q plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    probplot(data[column], dist=\"norm\", plot=plt)\n",
    "    \n",
    "    plt.title(f'Q-Q Plot for {column}')\n",
    "    \n",
    "    # Shapiro-Wilk test\n",
    "    stat, p = shapiro(data[column])\n",
    "    \n",
    "    print(f'Shapiro-Wilk Test for {column}: Stat={stat}, p={p}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    Transformations to Improve Normality\n",
    "If a feature exhibits non-normality, you can apply transformations such as log, square root, or Box-Cox transformations to improve its normality.\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# Apply log transformation\n",
    "data_log = data.copy()\n",
    "\n",
    "for column in data.columns:\n",
    "    \n",
    "    if any(data[column] <= 0):\n",
    "        \n",
    "        # Skip log transformation if there are non-positive values\n",
    "        continue\n",
    "        \n",
    "    data_log[column] = np.log(data[column])\n",
    "\n",
    "# Apply square root transformation\n",
    "data_sqrt = data.copy()\n",
    "\n",
    "for column in data.columns:\n",
    "    \n",
    "    data_sqrt[column] = np.sqrt(data[column])\n",
    "\n",
    "# Apply Box-Cox transformation\n",
    "data_boxcox = data.copy()\n",
    "\n",
    "for column in data.columns:\n",
    "    \n",
    "    # Box-Cox requires all positive values, so add a constant if necessary\n",
    "    \n",
    "    if any(data[column] <= 0):\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    data_boxcox[column], _ = boxcox(data[column])\n",
    "\n",
    "# Visualize the transformed features\n",
    "for column in data.columns:\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    \n",
    "    sns.histplot(data_log[column], kde=True)\n",
    "    \n",
    "    plt.title(f'Log Transformed: {column}')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    \n",
    "    sns.histplot(data_sqrt[column], kde=True)\n",
    "    \n",
    "    plt.title(f'Square Root Transformed: {column}')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    \n",
    "    sns.histplot(data_boxcox[column], kde=True)\n",
    "    \n",
    "    plt.title(f'Box-Cox Transformed: {column}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0daf0015-1190-485c-8a4e-4231b55ccb70",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Q6. Using the wine quality data set, perform principal component analysis (PCA) to reduce the number of \n",
    "# features. What is the minimum number of principal components required to explain 90% of the variance in \n",
    "# the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da4d83-0b91-44ef-871b-e3cdc7d1a440",
   "metadata": {},
   "source": [
    "ANS = To perform Principal Component Analysis (PCA) on the wine quality dataset and determine the minimum number of principal components required to explain 90% of the variance, you can follow these steps:\n",
    "\n",
    "Load the dataset: First, make sure to load the dataset.\n",
    "\n",
    "Standardize the data: PCA is affected by the scale of the data, so standardizing the features is important.\n",
    "\n",
    "Apply PCA: Perform PCA to reduce the number of features.\n",
    "\n",
    "Determine the number of components: Calculate the cumulative variance explained by the principal components and find the minimum number of components required to explain 90% of the variance.\n",
    "Here is a step-by-step implementation in Python:\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('winequality.csv')\n",
    "\n",
    "# Standardize the features\n",
    "features = data.drop('quality', axis=1)  # Assuming 'quality' is the target variable\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(scaled_features)\n",
    "\n",
    "# Calculate the cumulative variance explained\n",
    "cumulative_variance_explained = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot the cumulative variance explained\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(range(1, len(cumulative_variance_explained) + 1), cumulative_variance_explained, marker='o')\n",
    "\n",
    "plt.axhline(y=0.90, color='r', linestyle='--')\n",
    "\n",
    "plt.xlabel('Number of Principal Components')\n",
    "\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "\n",
    "plt.title('Cumulative Variance Explained by Principal Components')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Determine the number of components required to explain 90% of the variance\n",
    "num_components = np.argmax(cumulative_variance_explained >= 0.90) + 1\n",
    "\n",
    "print(f'The minimum number of principal components required to explain 90% of the variance is {num_components}.')\n",
    "\n",
    "Explanation:\n",
    "Loading the dataset: Replace 'winequality.csv' with the correct path to your dataset.\n",
    "Standardizing the data: Standardize the features to have a mean of 0 and a standard deviation of 1, which is crucial for PCA.\n",
    "Applying PCA: Perform PCA on the standardized data.\n",
    "Cumulative variance explained: Calculate the cumulative variance explained by the principal components.\n",
    "Plotting: Plot the cumulative variance explained to visualize how many components are needed to reach 90% of the variance.\n",
    "Determine the number of components: Find the minimum number of principal components required to explain at least 90% of the variance.\n",
    "By running this code, you will be able to determine the number of principal components needed to explain 90% of the variance in the wine quality dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd7b20-ee0b-4f00-9952-9454d6b9c9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
